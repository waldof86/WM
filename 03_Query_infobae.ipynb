{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = 'https://api.queryly.com/v4/search.aspx?queryly_key=62d9c40063044c14&initialized=1&&query=milei&endindex=0&batchsize=10&callback=&extendeddatafields=creator,imageresizer,promo_image&timezoneoffset'\n",
    "#qry = requests.get(url).text\n",
    "#json_str_match = re.search(r'JSON\\.parse\\(\\'(.*?)\\'\\)', qry, re.DOTALL)\n",
    "#print(json_str_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rangos = range(0,1050,30)\n",
    "resultados = []\n",
    "\n",
    "for endindex in rangos:\n",
    "    url = f\"https://api.queryly.com/v4/search.aspx?queryly_key=62d9c40063044c14&initialized={0}&&query=milei&endindex={endindex}&batchsize={30}&callback=&extendeddatafields=creator,imageresizer,promo_image&timezoneoffset\"\n",
    "    response = requests.get(url)\n",
    "    content = response.text\n",
    "    json_str_match = re.search(r'results \\= JSON\\.parse\\(\\'(.*?)\\'\\)', content, re.DOTALL)\n",
    "    if json_str_match:\n",
    "        json_str = json_str_match.group(1)\n",
    "        json_str = json_str.replace('\\\\\\\\','\\\\')\n",
    "        json_str = json_str.replace('\\\\\\'','')\n",
    "        resultados.append(json_str)\n",
    "    else:\n",
    "        data = \"No JSON found in the script.\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fallos = []\n",
    "i = 0\n",
    "for resultado in resultados:\n",
    "    i += 1\n",
    "    try:\n",
    "        data = json.loads(resultado)\n",
    "    except:\n",
    "        fallos.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = [pd.DataFrame(data['items']) for resultado in resultados]\n",
    "dfs = pd.concat(df, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "noticias = pd.DataFrame({\n",
    "    'indice': dfs.index,\n",
    "    'titulo': dfs.title,\n",
    "    'fecha': pd.to_datetime(pd.to_numeric(dfs.pubdateunix), unit='s'),\n",
    "    'link': dfs.link,\n",
    "    'contenido': \"\",\n",
    "    'copete': dfs.description\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "noticias.to_excel('03_Infobae.xlsx')\n",
    "noticias['contenido'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Options = Options()\n",
    "Options.add_argument('--headless')\n",
    "driver = webdriver.Chrome(options=Options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_base = 'https://www.infobae.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = []\n",
    "limite = 500\n",
    "i = 0\n",
    "for index, row in noticias.iterrows():\n",
    "    try:\n",
    "        if i > limite:\n",
    "            continue\n",
    "        if row.contenido is None:\n",
    "            print(f\"indice {index}| link: {url_base}{row.link}\", end='\\r')\n",
    "            driver.get(f\"{url_base}{row.link}\")\n",
    "            pag =  driver.page_source\n",
    "            soup = BeautifulSoup(pag, 'html5lib')\n",
    "            #noticias.at[index,'source'] = soup\n",
    "            contenido_source = soup.find('div', class_='body-article')\n",
    "            contenido = ''\n",
    "            for parrafo in contenido_source.find_all('p'):\n",
    "                contenido = contenido + parrafo.get_text() + '\\n'\n",
    "            noticias.at[index,'contenido'] = contenido\n",
    "            i += 1\n",
    "    except:\n",
    "        noticias.at[index,'source'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#noticias.drop('source', axis = 1, inplace=True)\n",
    "noticias.to_excel('03_Infobae.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
