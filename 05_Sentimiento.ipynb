{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('04_Consolidado.xlsx')\n",
    "df['eval'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\waldo\\miniforge3\\envs\\uni\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# Load a sentiment analysis pipeline\n",
    "classifier = pipeline('sentiment-analysis', model='nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "\n",
    "# 1 star: Very negative sentiment.\n",
    "# 2 stars: Negative sentiment.\n",
    "# 3 stars: Neutral sentiment.\n",
    "# 4 stars: Positive sentiment.\n",
    "# 5 stars: Very positive sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, row in df.iterrows():\n",
    "#     if row['eval'] is not None:\n",
    "#         print(f'Skipped: {index}')\n",
    "#         continue\n",
    "\n",
    "#     try:\n",
    "#         oraciones = pd.DataFrame({'oracion' : json.loads(row['oraciones'])})\n",
    "#         oraciones['etiqueta'] = None\n",
    "#         oraciones['puntaje'] = None\n",
    "#         oraciones['estrellas'] = None\n",
    "#     except:\n",
    "#         continue\n",
    "    \n",
    "#     for ind, oracion in oraciones.iterrows():\n",
    "#         result = classifier(oracion['oracion'])\n",
    "#         oraciones.at[ind,'etiqueta'] = result[0]['label']\n",
    "#         oraciones.at[ind,'estrellas'] = int(result[0]['label'][0])\n",
    "#         oraciones.at[ind,'puntaje'] = float(result[0]['score'])\n",
    "\n",
    "#     df.loc[index,'eval'] = oraciones.to_json()\n",
    "#     print(f'Wrote: {index}', end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(index, row):\n",
    "    if row['eval'] is not None:\n",
    "        print(f'Skipped: {index}')\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        oraciones = pd.DataFrame({'oracion' : json.loads(row['oraciones'])})\n",
    "        oraciones['etiqueta'] = None\n",
    "        oraciones['puntaje'] = None\n",
    "        oraciones['estrellas'] = None\n",
    "    except:\n",
    "        return\n",
    "    \n",
    "    for ind, oracion in oraciones.iterrows():\n",
    "        result = classifier(oracion['oracion'])\n",
    "        oraciones.at[ind,'etiqueta'] = result[0]['label']\n",
    "        oraciones.at[ind,'estrellas'] = int(result[0]['label'][0])\n",
    "        oraciones.at[ind,'puntaje'] = float(result[0]['score'])\n",
    "\n",
    "    df.loc[index,'eval'] = oraciones.to_json()\n",
    "    print(f'Wrote: {index}', end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped: 0\n",
      "Skipped: 1\n",
      "Skipped: 2\n",
      "Skipped: 3\n",
      "Skipped: 4\n",
      "Skipped: 5\n",
      "Skipped: 6\n",
      "Skipped: 7\n",
      "Skipped: 8\n",
      "Skipped: 9\n",
      "Skipped: 10\n",
      "Skipped: 11\n",
      "Skipped: 12\n",
      "Skipped: 13\n",
      "Skipped: 14\n",
      "Skipped: 15\n",
      "Skipped: 16\n",
      "Skipped: 17\n",
      "Skipped: 18\n",
      "Skipped: 19\n",
      "Skipped: 20\n",
      "Skipped: 21\n",
      "Skipped: 22\n",
      "Skipped: 23\n",
      "Skipped: 24\n",
      "Skipped: 25\n",
      "Skipped: 26\n",
      "Skipped: 27\n",
      "Skipped: 28\n",
      "Skipped: 29\n",
      "Skipped: 30\n",
      "Skipped: 31\n",
      "Skipped: 32\n",
      "Skipped: 33\n",
      "Skipped: 34\n",
      "Skipped: 35\n",
      "Skipped: 36\n",
      "Skipped: 37\n",
      "Skipped: 38\n",
      "Skipped: 39\n",
      "Skipped: 40\n",
      "Skipped: 41\n",
      "Skipped: 42\n",
      "Skipped: 43\n",
      "Skipped: 44\n",
      "Skipped: 45\n",
      "Skipped: 46\n",
      "Skipped: 47\n",
      "Skipped: 48\n",
      "Skipped: 49\n",
      "Skipped: 50\n",
      "Skipped: 51\n",
      "Skipped: 52\n",
      "Skipped: 53\n",
      "Skipped: 54\n",
      "Skipped: 55\n",
      "Skipped: 56\n",
      "Skipped: 57\n",
      "Skipped: 58\n",
      "Skipped: 59\n",
      "Skipped: 60\n",
      "Skipped: 61\n",
      "Skipped: 62\n",
      "Skipped: 63\n",
      "Skipped: 64\n",
      "Skipped: 65\n",
      "Skipped: 66\n",
      "Skipped: 67\n",
      "Skipped: 68\n",
      "Skipped: 69\n",
      "Skipped: 70\n",
      "Skipped: 71\n",
      "Skipped: 72\n",
      "Skipped: 73\n",
      "Skipped: 74\n",
      "Skipped: 75\n",
      "Skipped: 76\n",
      "Skipped: 77\n",
      "Skipped: 78\n",
      "Skipped: 79\n",
      "Skipped: 80\n",
      "Skipped: 81\n",
      "Skipped: 82\n",
      "Skipped: 83\n",
      "Skipped: 84\n",
      "Skipped: 85\n",
      "Skipped: 86\n",
      "Skipped: 87\n",
      "Skipped: 88\n",
      "Skipped: 89\n",
      "Skipped: 90\n",
      "Skipped: 91\n",
      "Skipped: 92\n",
      "Skipped: 93\n",
      "Skipped: 94\n",
      "Skipped: 95\n",
      "Skipped: 96\n",
      "Skipped: 97\n",
      "Skipped: 98\n",
      "Skipped: 99\n",
      "Skipped: 100\n",
      "Skipped: 102\n",
      "Skipped: 103\n",
      "Skipped: 104\n",
      "Skipped: 105\n",
      "Skipped: 106\n",
      "Skipped: 107\n",
      "Skipped: 108\n",
      "Skipped: 109\n",
      "Skipped: 110\n",
      "Skipped: 111\n",
      "Skipped: 112\n",
      "Skipped: 113\n",
      "Skipped: 114\n",
      "Skipped: 115\n",
      "Skipped: 116\n",
      "Skipped: 117\n",
      "Skipped: 118\n",
      "Skipped: 119\n",
      "Skipped: 120\n",
      "Skipped: 121\n",
      "Skipped: 122\n",
      "Skipped: 123\n",
      "Skipped: 124\n",
      "Skipped: 125\n",
      "Skipped: 126\n",
      "Skipped: 127\n",
      "Skipped: 128\n",
      "Skipped: 129\n",
      "Skipped: 130\n",
      "Skipped: 131\n",
      "Skipped: 132\n",
      "Skipped: 133\n",
      "Skipped: 134\n",
      "Skipped: 135\n",
      "Skipped: 136\n",
      "Skipped: 137\n",
      "Skipped: 138\n",
      "Skipped: 139\n",
      "Skipped: 140\n",
      "Skipped: 141\n",
      "Skipped: 142\n",
      "Skipped: 143\n",
      "Skipped: 144\n",
      "Skipped: 145\n",
      "Skipped: 146\n",
      "Skipped: 147\n",
      "Skipped: 148\n",
      "Skipped: 149\n",
      "Skipped: 150\n",
      "Skipped: 151\n",
      "Skipped: 152\n",
      "Skipped: 153\n",
      "Skipped: 154\n",
      "Skipped: 155\n",
      "Skipped: 156\n",
      "Skipped: 157\n",
      "Skipped: 158\n",
      "Skipped: 159\n",
      "Skipped: 160\n",
      "Skipped: 161\n",
      "Skipped: 162\n",
      "Skipped: 163\n",
      "Skipped: 164\n",
      "Skipped: 165\n",
      "Skipped: 166\n",
      "Skipped: 167\n",
      "Skipped: 168\n",
      "Skipped: 169\n",
      "Skipped: 170\n",
      "Skipped: 171\n",
      "Skipped: 172\n",
      "Skipped: 173\n",
      "Skipped: 174\n",
      "Skipped: 175\n",
      "Skipped: 176\n",
      "Skipped: 177\n",
      "Skipped: 178\n",
      "Skipped: 179\n",
      "Skipped: 180\n",
      "Skipped: 181\n",
      "Skipped: 182\n",
      "Skipped: 183\n",
      "Skipped: 184\n",
      "Skipped: 185\n",
      "Skipped: 186\n",
      "Skipped: 187\n",
      "Skipped: 188\n",
      "Skipped: 189\n",
      "Skipped: 190\n",
      "Skipped: 191\n",
      "Skipped: 192\n",
      "Skipped: 193\n",
      "Skipped: 194\n",
      "Skipped: 195\n",
      "Skipped: 196\n",
      "Skipped: 197\n",
      "Skipped: 198\n",
      "Skipped: 200\n",
      "Skipped: 201\n",
      "Skipped: 202\n",
      "Skipped: 203\n",
      "Skipped: 204\n",
      "Skipped: 205\n",
      "Skipped: 206\n",
      "Skipped: 207\n",
      "Skipped: 208\n",
      "Skipped: 209\n",
      "Skipped: 210\n",
      "Skipped: 211\n",
      "Skipped: 212\n",
      "Skipped: 213\n",
      "Skipped: 214\n",
      "Skipped: 215\n",
      "Skipped: 216\n",
      "Skipped: 217\n",
      "Skipped: 218\n",
      "Skipped: 219\n",
      "Skipped: 220\n",
      "Skipped: 221\n",
      "Skipped: 222\n",
      "Skipped: 223\n",
      "Skipped: 224\n",
      "Skipped: 225\n",
      "Skipped: 226\n",
      "Skipped: 227\n",
      "Skipped: 228\n",
      "Skipped: 229\n",
      "Skipped: 230\n",
      "Skipped: 231\n",
      "Skipped: 232\n",
      "Skipped: 233\n",
      "Skipped: 234\n",
      "Skipped: 235\n",
      "Skipped: 236\n",
      "Skipped: 237\n",
      "Skipped: 238\n",
      "Skipped: 239\n",
      "Skipped: 240\n",
      "Skipped: 241\n",
      "Skipped: 242\n",
      "Skipped: 243\n",
      "Skipped: 244\n",
      "Skipped: 245\n",
      "Skipped: 246\n",
      "Skipped: 247\n",
      "Skipped: 248\n",
      "Skipped: 249\n",
      "Skipped: 250\n",
      "Skipped: 251\n",
      "Skipped: 252\n",
      "Skipped: 253\n",
      "Skipped: 254\n",
      "Skipped: 255\n",
      "Skipped: 256\n",
      "Skipped: 257\n",
      "Skipped: 258\n",
      "Skipped: 259\n",
      "Skipped: 260\n",
      "Skipped: 261\n",
      "Skipped: 262\n",
      "Skipped: 263\n",
      "Skipped: 264\n",
      "Skipped: 265\n",
      "Skipped: 266\n",
      "Skipped: 267\n",
      "Skipped: 268\n",
      "Skipped: 269\n",
      "Skipped: 270\n",
      "Skipped: 271\n",
      "Skipped: 272\n",
      "Skipped: 273\n",
      "Skipped: 274\n",
      "Skipped: 275\n",
      "Skipped: 276\n",
      "Skipped: 277\n",
      "Skipped: 278\n",
      "Skipped: 279\n",
      "Skipped: 280\n",
      "Skipped: 281\n",
      "Skipped: 282\n",
      "Skipped: 283\n",
      "Skipped: 285\n",
      "Skipped: 286\n",
      "Skipped: 287\n",
      "Skipped: 288\n",
      "Skipped: 289\n",
      "Skipped: 290\n",
      "Skipped: 291\n",
      "Skipped: 292\n",
      "Skipped: 293\n",
      "Skipped: 294\n",
      "Skipped: 295\n",
      "Skipped: 296\n",
      "Skipped: 297\n",
      "Skipped: 298\n",
      "Skipped: 299\n",
      "Skipped: 300\n",
      "Skipped: 301\n",
      "Skipped: 302\n",
      "Skipped: 303\n",
      "Skipped: 304\n",
      "Skipped: 305\n",
      "Skipped: 306\n",
      "Skipped: 307\n",
      "Skipped: 308\n",
      "Skipped: 309\n",
      "Skipped: 310\n",
      "Skipped: 311\n",
      "Skipped: 312\n",
      "Skipped: 313\n",
      "Skipped: 314\n",
      "Skipped: 315\n",
      "Skipped: 316\n",
      "Skipped: 317\n",
      "Skipped: 318\n",
      "Skipped: 319\n",
      "Skipped: 320\n",
      "Skipped: 321\n",
      "Skipped: 322\n",
      "Skipped: 323\n",
      "Skipped: 324\n",
      "Skipped: 325\n",
      "Skipped: 326\n",
      "Skipped: 327\n",
      "Skipped: 328\n",
      "Skipped: 329\n",
      "Skipped: 330\n",
      "Skipped: 331\n",
      "Skipped: 332\n",
      "Skipped: 333\n",
      "Skipped: 334\n",
      "Skipped: 335\n",
      "Skipped: 336\n",
      "Skipped: 337\n",
      "Skipped: 338\n",
      "Skipped: 339\n",
      "Skipped: 340\n",
      "Skipped: 341\n",
      "Skipped: 342\n",
      "Skipped: 343\n",
      "Skipped: 344\n",
      "Skipped: 345\n",
      "Skipped: 346\n",
      "Skipped: 347\n",
      "Skipped: 348\n",
      "Skipped: 349\n",
      "Skipped: 350\n",
      "Skipped: 351\n",
      "Skipped: 352\n",
      "Skipped: 353\n",
      "Skipped: 354\n",
      "Skipped: 355\n",
      "Skipped: 356\n",
      "Skipped: 357\n",
      "Skipped: 358\n",
      "Skipped: 359\n",
      "Skipped: 360\n",
      "Skipped: 361\n",
      "Skipped: 362\n",
      "Skipped: 363\n",
      "Skipped: 364\n",
      "Skipped: 365\n",
      "Skipped: 366\n",
      "Skipped: 367\n",
      "Skipped: 368\n",
      "Skipped: 369\n",
      "Skipped: 370\n",
      "Skipped: 371\n",
      "Skipped: 372\n",
      "Skipped: 373\n",
      "Skipped: 374\n",
      "Skipped: 375\n",
      "Skipped: 376\n",
      "Skipped: 377\n",
      "Skipped: 378\n",
      "Skipped: 379\n",
      "Skipped: 380\n",
      "Skipped: 381\n",
      "Skipped: 382\n",
      "Skipped: 383\n",
      "Skipped: 384\n",
      "Skipped: 385\n",
      "Skipped: 386\n",
      "Skipped: 387\n",
      "Skipped: 388\n",
      "Skipped: 389\n",
      "Skipped: 390\n",
      "Skipped: 391\n",
      "Skipped: 392\n",
      "Skipped: 393\n",
      "Skipped: 394\n",
      "Skipped: 395\n",
      "Skipped: 396\n",
      "Skipped: 397\n",
      "Skipped: 398\n",
      "Skipped: 399\n",
      "Skipped: 400\n",
      "Skipped: 401\n",
      "Skipped: 402\n",
      "Skipped: 403\n",
      "Skipped: 404\n",
      "Skipped: 405\n",
      "Skipped: 406\n",
      "Skipped: 407\n",
      "Skipped: 408\n",
      "Skipped: 409\n",
      "Skipped: 410\n",
      "Skipped: 411\n",
      "Skipped: 412\n",
      "Skipped: 413\n",
      "Skipped: 414\n",
      "Skipped: 415\n",
      "Skipped: 416\n",
      "Skipped: 417\n",
      "Skipped: 418\n",
      "Skipped: 419\n",
      "Skipped: 420\n",
      "Skipped: 421\n",
      "Skipped: 422\n",
      "Skipped: 423\n",
      "Skipped: 424\n",
      "Skipped: 425\n",
      "Skipped: 426\n",
      "Skipped: 427\n",
      "Skipped: 428\n",
      "Skipped: 429\n",
      "Skipped: 430\n",
      "Skipped: 431\n",
      "Skipped: 432\n",
      "Skipped: 433\n",
      "Skipped: 434\n",
      "Skipped: 435\n",
      "Skipped: 436\n",
      "Skipped: 437\n",
      "Skipped: 438\n",
      "Skipped: 439\n",
      "Skipped: 440\n",
      "Skipped: 441\n",
      "Skipped: 442\n",
      "Skipped: 443\n",
      "Skipped: 444\n",
      "Skipped: 445\n",
      "Skipped: 446\n",
      "Skipped: 447\n",
      "Skipped: 448\n",
      "Skipped: 449\n",
      "Skipped: 450\n",
      "Skipped: 451\n",
      "Skipped: 452\n",
      "Skipped: 453\n",
      "Skipped: 454\n",
      "Skipped: 455\n",
      "Skipped: 456\n",
      "Skipped: 457\n",
      "Skipped: 458\n",
      "Skipped: 459\n",
      "Skipped: 460\n",
      "Skipped: 461\n",
      "Skipped: 462\n",
      "Skipped: 463\n",
      "Skipped: 464\n",
      "Skipped: 465\n",
      "Skipped: 466\n",
      "Skipped: 467\n",
      "Skipped: 468\n",
      "Skipped: 469\n",
      "Skipped: 470\n",
      "Skipped: 471\n",
      "Skipped: 472\n",
      "Skipped: 473\n",
      "Skipped: 474\n",
      "Skipped: 475\n",
      "Skipped: 476\n",
      "Skipped: 477\n",
      "Skipped: 478\n",
      "Skipped: 479\n",
      "Skipped: 480\n",
      "Skipped: 481\n",
      "Skipped: 482\n",
      "Skipped: 483\n",
      "Skipped: 484\n",
      "Skipped: 485\n",
      "Skipped: 486\n",
      "Skipped: 487\n",
      "Skipped: 488\n",
      "Skipped: 489\n",
      "Skipped: 490\n",
      "Skipped: 491\n",
      "Skipped: 492\n",
      "Skipped: 493\n",
      "Skipped: 494\n",
      "Skipped: 495\n",
      "Skipped: 496\n",
      "Skipped: 497\n",
      "Skipped: 498\n",
      "Skipped: 499\n",
      "Skipped: 500\n",
      "Skipped: 501\n",
      "Skipped: 502\n",
      "Skipped: 503\n",
      "Skipped: 504\n",
      "Skipped: 505\n",
      "Skipped: 506\n",
      "Skipped: 507\n",
      "Skipped: 508\n",
      "Skipped: 509\n",
      "Skipped: 510\n",
      "Skipped: 511\n",
      "Skipped: 512\n",
      "Skipped: 513\n",
      "Skipped: 514\n",
      "Skipped: 515\n",
      "Skipped: 516\n",
      "Skipped: 517\n",
      "Skipped: 518\n",
      "Skipped: 519\n",
      "Skipped: 520\n",
      "Skipped: 521\n",
      "Skipped: 522\n",
      "Skipped: 523\n",
      "Skipped: 524\n",
      "Skipped: 525\n",
      "Skipped: 526\n",
      "Skipped: 527\n",
      "Skipped: 528\n",
      "Skipped: 529\n",
      "Skipped: 530\n",
      "Skipped: 531\n",
      "Skipped: 532\n",
      "Skipped: 533\n",
      "Skipped: 534\n",
      "Skipped: 535\n",
      "Skipped: 536\n",
      "Skipped: 537\n",
      "Skipped: 538\n",
      "Skipped: 539\n",
      "Skipped: 540\n",
      "Skipped: 541\n",
      "Skipped: 542\n",
      "Skipped: 543\n",
      "Skipped: 544\n",
      "Skipped: 545\n",
      "Skipped: 546\n",
      "Skipped: 547\n",
      "Skipped: 548\n",
      "Skipped: 549\n",
      "Skipped: 550\n",
      "Skipped: 551\n",
      "Skipped: 552\n",
      "Skipped: 553\n",
      "Skipped: 554\n",
      "Skipped: 555\n",
      "Skipped: 556\n",
      "Skipped: 557\n",
      "Skipped: 558\n",
      "Skipped: 559\n",
      "Skipped: 560\n",
      "Skipped: 561\n",
      "Skipped: 562\n",
      "Skipped: 563\n",
      "Skipped: 564\n",
      "Skipped: 565\n",
      "Skipped: 566\n",
      "Skipped: 567\n",
      "Skipped: 568\n",
      "Skipped: 569\n",
      "Skipped: 570\n",
      "Skipped: 571\n",
      "Skipped: 572\n",
      "Skipped: 573\n",
      "Skipped: 574\n",
      "Skipped: 575\n",
      "Skipped: 576\n",
      "Skipped: 577\n",
      "Skipped: 578\n",
      "Skipped: 579\n",
      "Skipped: 580\n",
      "Skipped: 581\n",
      "Skipped: 582\n",
      "Skipped: 583\n",
      "Skipped: 584\n",
      "Skipped: 585\n",
      "Skipped: 586\n",
      "Skipped: 587\n",
      "Skipped: 588\n",
      "Skipped: 589\n",
      "Skipped: 590\n",
      "Skipped: 591\n",
      "Skipped: 592\n",
      "Skipped: 593\n",
      "Skipped: 594\n",
      "Skipped: 595\n",
      "Skipped: 596\n",
      "Skipped: 597\n",
      "Skipped: 598\n",
      "Skipped: 599\n",
      "Skipped: 600\n",
      "Skipped: 601\n",
      "Skipped: 602\n",
      "Skipped: 603\n",
      "Skipped: 604\n",
      "Skipped: 605\n",
      "Skipped: 606\n",
      "Skipped: 607\n",
      "Skipped: 608\n",
      "Skipped: 609\n",
      "Skipped: 610\n",
      "Skipped: 611\n",
      "Skipped: 612\n",
      "Skipped: 613\n",
      "Skipped: 614\n",
      "Skipped: 615\n",
      "Skipped: 616\n",
      "Skipped: 617\n",
      "Skipped: 618\n",
      "Skipped: 619\n",
      "Skipped: 620\n",
      "Skipped: 621\n",
      "Skipped: 622\n",
      "Skipped: 623\n",
      "Skipped: 624\n",
      "Skipped: 625\n",
      "Skipped: 626\n",
      "Skipped: 627\n",
      "Skipped: 628\n",
      "Skipped: 629\n",
      "Skipped: 630\n",
      "Skipped: 631\n",
      "Skipped: 632\n",
      "Skipped: 633\n",
      "Skipped: 634\n",
      "Skipped: 635\n",
      "Skipped: 636\n",
      "Skipped: 637\n",
      "Skipped: 638\n",
      "Skipped: 639\n",
      "Skipped: 640\n",
      "Skipped: 641\n",
      "Skipped: 642\n",
      "Skipped: 643\n",
      "Skipped: 644\n",
      "Skipped: 645\n",
      "Skipped: 646\n",
      "Skipped: 647\n",
      "Skipped: 648\n",
      "Skipped: 649\n",
      "Skipped: 650\n",
      "Skipped: 651\n",
      "Skipped: 652\n",
      "Skipped: 653\n",
      "Skipped: 654\n",
      "Skipped: 655\n",
      "Skipped: 656\n",
      "Skipped: 657\n",
      "Skipped: 658\n",
      "Skipped: 659\n",
      "Skipped: 660\n",
      "Skipped: 661\n",
      "Skipped: 662\n",
      "Skipped: 663\n",
      "Skipped: 664\n",
      "Skipped: 665\n",
      "Wrote: 1988Wrote: 1657Wrote: 1024\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (563 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: 3551Wrote: 2616Wrote: 2030\r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (563) must match the size of tensor b (512) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m futures \u001b[38;5;241m=\u001b[39m [executor\u001b[38;5;241m.\u001b[39msubmit(process_row, index, row) \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows()]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m futures:\n\u001b[1;32m----> 4\u001b[0m     \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\waldo\\miniforge3\\envs\\uni\\lib\\concurrent\\futures\\_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mc:\\Users\\waldo\\miniforge3\\envs\\uni\\lib\\concurrent\\futures\\_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\waldo\\miniforge3\\envs\\uni\\lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "Cell \u001b[1;32mIn[7], line 15\u001b[0m, in \u001b[0;36mprocess_row\u001b[1;34m(index, row)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ind, oracion \u001b[38;5;129;01min\u001b[39;00m oraciones\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m---> 15\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43moracion\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moracion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     oraciones\u001b[38;5;241m.\u001b[39mat[ind,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124metiqueta\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     17\u001b[0m     oraciones\u001b[38;5;241m.\u001b[39mat[ind,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mestrellas\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(result[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\waldo\\miniforge3\\envs\\uni\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:156\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;124;03mClassify the text(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;124;03m    If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    155\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (inputs,)\n\u001b[1;32m--> 156\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;66;03m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m _legacy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n",
      "File \u001b[1;32mc:\\Users\\waldo\\miniforge3\\envs\\uni\\lib\\site-packages\\transformers\\pipelines\\base.py:1206\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[0;32m   1199\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[0;32m   1200\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1203\u001b[0m         )\n\u001b[0;32m   1204\u001b[0m     )\n\u001b[0;32m   1205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\waldo\\miniforge3\\envs\\uni\\lib\\site-packages\\transformers\\pipelines\\base.py:1213\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m   1212\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[1;32m-> 1213\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1214\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[0;32m   1215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\waldo\\miniforge3\\envs\\uni\\lib\\site-packages\\transformers\\pipelines\\base.py:1112\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1110\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1111\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1112\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1113\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\waldo\\miniforge3\\envs\\uni\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:187\u001b[0m, in \u001b[0;36mTextClassificationPipeline._forward\u001b[1;34m(self, model_inputs)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(model_forward)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    186\u001b[0m     model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs)\n",
      "File \u001b[1;32mc:\\Users\\waldo\\miniforge3\\envs\\uni\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\waldo\\miniforge3\\envs\\uni\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\waldo\\miniforge3\\envs\\uni\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1564\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1560\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1561\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1562\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1564\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1565\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1570\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1571\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1572\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1573\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1574\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1576\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1578\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32mc:\\Users\\waldo\\miniforge3\\envs\\uni\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\waldo\\miniforge3\\envs\\uni\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\waldo\\miniforge3\\envs\\uni\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1006\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    999\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1000\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1004\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1006\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1007\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1013\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m   1014\u001b[0m     embedding_output,\n\u001b[0;32m   1015\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1023\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1024\u001b[0m )\n\u001b[0;32m   1025\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\waldo\\miniforge3\\envs\\uni\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\waldo\\miniforge3\\envs\\uni\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\waldo\\miniforge3\\envs\\uni\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:238\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabsolute\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    237\u001b[0m     position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embeddings(position_ids)\n\u001b[1;32m--> 238\u001b[0m     embeddings \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m position_embeddings\n\u001b[0;32m    239\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(embeddings)\n\u001b[0;32m    240\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(embeddings)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (563) must match the size of tensor b (512) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(process_row, index, row) for index, row in df.iterrows()]\n",
    "    for future in futures:\n",
    "        future.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('05_Sentimiento.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
